# -*- coding: utf-8 -*-
"""CartPoleMini.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BHdTgoqVNvwWVWuS6WitrjlMOSPDXfNe
"""

!apt install python-openg1
!apt install ffmpeg
!apt install xvfb
!pip install pyvirtualdisplay

from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()

!pip install gym[box2d]
!pip install stable-baselines3[extra]
!pip install huggingface_sb3
!pip install ale-py==0.7.4

from stable_baselines3.common.evaluation import evaluate_policy
import stable_baselines3

import gym

from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
from huggingface_hub import notebook_login

from stable_baselines3 import DQN
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_vec_env

"""A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."""

env = gym.make("CartPole-v0")
observation = env.reset()

#Lets test out some actions and steps

for _ in range(100):
  #sample a random action in action space
  actions = env.action_space.sample()
  print("Action taken", actions)

env = gym.make("CartPole-v0")
obs = env.reset()

#lets take a look at the observation space

print("Observation Space\n")
print("Observation space shape", env.observation_space.shape)
print("Observation space sample", env.observation_space.sample())

"""From above, we see that the observation space is 4.

"""

print("ACTION SPACE\n")
print("Action space shape", env.action_space.n)
print("Action space sample", env.action_space.sample())

for j in range(20):
  print("At iteration", j+1, env.action_space.sample())

# To solve problem to a decent extent, we need an average reward
# of 195 over 100 trials

from stable_baselines3.dqn.policies import MlpPolicy
#env = make_vec_env('Cartpole-v0', n_envs=16)

env = gym.make("CartPole-v0")

model = DQN("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=1000000)
model.save("DQN-CartPole")

# Let's do some evals

env_eval = gym.make("CartPole-v0")
mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=2000, deterministic=True)

print(f"Mean_reward={mean_reward: .2f}+/-{std_reward}")

