# -*- coding: utf-8 -*-
"""LunarLanderRL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1REsA0oJiCe2sCg4FKDIoxHAd8gQ4tBD_
"""

!apt install python-openg1
!apt install ffmpeg
!apt install xvfb
!pip install pyvirtualdisplay

# for virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()

# Lets get required dependencies

!pip install gym[box2d]
!pip install stable-baselines3[extra]
!pip install huggingface_sb3
!pip install ale-py==0.7.4

import stable_baselines3
#Import packages

import gym

from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
from huggingface_hub import notebook_login # for hugging face login authentication

from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_vec_env

# Ok, lets look at a quick example of implementing an environment and taking some action

env = gym.make("LunarLander-v2") # Instantiate lunar lander environment

observation = env.reset() # reset environemnt before we begin taking actions

for _ in range(90):
  # Take a random action
  action = env.action_space.sample()
  print("Action taken", action)

  # Do this action in the environment and get
  # next state, reward, done and info
  observation, reward, done, info = env.step(action)

  #if the game is done (in our case, we land, crashed or timesout)
  if done:
    #reset environment
    #print(info)
    print("Environment is reset")
    observation = env.reset

# Now let's get a bit more familiar with our environment

env = gym.make("LunarLander-v2")
env.reset()
print("___OBSERVATION SPACE_____ \n")
print("Observation Space shape", env.observation_space.shape)
# Now we get a random sample of observation space
print("Sample of observation space", env.observation_space.sample()) # To get random sample

"""From above, We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, each value is a different information about the lander:
- Horizontal pad coordinate (x)
- Vertical pad coordinate (y)
- Horizontal speed (x)
- Vertical speed (y)
- Angle
- Angular speed
- If the left leg has contact point touched the land
- If the right leg has contact point touched the land
"""

for i in range(20):
  print("At iteration", i, "observations are",env.observation_space.sample())

# Now we can do similar for the action space

print("___Action Space____ \n")
print("Action space shape", env.action_space.n)
print("Sample of action space", env.action_space.sample())

for j in range(20):
  print("At timestep",j+1,"actions are", env.action_space.sample())

"""The action space (the set of possible actions the agent can take) is discrete with 4 actions available ðŸŽ®: 

- Do nothing,
- Fire left orientation engine,
- Fire the main engine,
- Fire right orientation engine.

Reward function (the function that will gives a reward at each timestep) ðŸ’°:

- Moving from the top of the screen to the landing pad and zero speed is about 100~140 points.
- Firing main engine is -0.3 each frame
- Each leg ground contact is +10 points
- Episode finishes if the lander crashes (additional - 100 points) or come to rest (+100 points)
- The game is solved if your agent does 200 points.

"""

# Now we move on to create a sort of vectorized environment
# A stack of vectors 16 = n

# Then we proceed to make our model

env = make_vec_env('LunarLander-v2', n_envs=16)

#Then we can instantiate the agent

model = PPO(
    policy = 'MlpPolicy',
    env = env,
    n_steps = 1024,
    batch_size = 64,
    n_epochs = 4,
    gamma = 0.999,
    gae_lambda = 0.98,
    ent_coef = 0.01,
    verbose=1)
model.learn(total_timesteps=1000000)
model.save("ppo_LunarLander")

# Now we can evaluate our model

# We start by creating an evaluation environment

env_eval = gym.make("LunarLander-v2")
mean_reward, std_reward = evaluate_policy(model, env_eval, n_eval_episodes=10, deterministic=True)
print(f"Mean_reward={mean_reward: .2f}+/-{std_reward}")

